{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yujin\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import contractions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.metrics import Recall, Precision\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    # split into words\n",
    "    tokens = word_tokenize(expanded_text)\n",
    "    # convert to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # remove punctuation from each word\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with UTF-8 encoding to avoid UnicodeDecodeError\n",
    "def load_reviews(directory):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for label_type in ['pos', 'neg']:\n",
    "        label = 1 if label_type == 'pos' else 0  # 1 for positive, 0 for negative\n",
    "        dir_name = os.path.join(directory, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(dir_name, fname), 'r', encoding='utf-8') as file:  # Specify UTF-8 encoding\n",
    "                    reviews.append(preprocess_text(file.read()))  # Preprocess the text\n",
    "                    labels.append(label)\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess training and testing data\n",
    "train_reviews, train_labels = load_reviews('train/')\n",
    "test_reviews, test_labels = load_reviews('test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_reviews, train_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=10000)  # Limit vocabulary to 10,000 most common words\n",
    "tokenizer.fit_on_texts(X_train)  # Fit tokenizer on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sequences of integers\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to ensure they all have the same length\n",
    "maxlen = 300  # You can adjust this depending on the average review length\n",
    "X_train_pad = pad_sequences(X_train_seq , maxlen=maxlen)\n",
    "X_val_pad = pad_sequences(X_val_seq , maxlen=maxlen)\n",
    "X_test_pad = pad_sequences(X_test_seq , maxlen=maxlen)\n",
    "\n",
    "# Define vocab_size and embedding_dim\n",
    "vocab_size = 10000  # Matches the tokenizer's num_words\n",
    "embedding_dim = 100 # You can adjust this based on your needs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of Precision and Recall outside the metric function\n",
    "precision_metric = Precision(name='precision')\n",
    "recall_metric = Recall(name='recall')\n",
    "\n",
    "# Custom F1 score metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_metric(y_true, y_pred)\n",
    "    recall = recall_metric(y_true, y_pred)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to NumPy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, mask_zero=True, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))  # First Bidirectional LSTM\n",
    "model.add(Bidirectional(LSTM(32)))   # Second Bidirectional LSTM\n",
    "model.add(Dense(32,kernel_regularizer=regularizers.l2(0.00001), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "    metrics=['accuracy', precision_metric, recall_metric, f1_score]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "training_history = model.fit(\n",
    "    X_train_pad, y_train,\n",
    "    epochs= 10,\n",
    "    batch_size=512,\n",
    "    validation_data=(X_val_pad, y_val),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_labels = np.array(test_labels)\n",
    "score = model.evaluate(X_test_pad, test_labels, verbose=1)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f'Test score or loss: {score[0]:.4f}')   # Loss\n",
    "print(f'Test accuracy: {score[1]:.4f}')         # Accuracy\n",
    "print(f'Test Precision: {score[2]:.4f}')        # Precision\n",
    "print(f'Test Recall: {score[3]:.4f}')           # Recall\n",
    "print(f'Test F1 Score: {score[4]:.4f}')         # F1 Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(training_history.history['accuracy'])\n",
    "plt.plot(training_history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(training_history.history['loss'], label='Train')\n",
    "ax.plot(training_history.history['val_loss'], label='Validation')\n",
    "ax.legend()\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Model loss')\n",
    "ax.set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 score\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(training_history.history['f1_score'])\n",
    "ax.set_title('Training F1 Score')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('F1 Score')\n",
    "ax.legend(['F1 Score'], loc='upper left')\n",
    "ax.set_ylim(0.5, 1)\n",
    "plt.show()\n",
    "\n",
    "# Plot precision and recall\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(training_history.history['precision'])  \n",
    "ax.plot(training_history.history['recall'])    \n",
    "ax.set_title('Training Precision and Recall')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Metrics')\n",
    "ax.set_ylim(0.7, 1) \n",
    "ax.legend(['Precision', 'Recall'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
